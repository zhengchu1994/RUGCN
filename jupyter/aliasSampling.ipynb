{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alias算法\n",
    "* Alias算法的作用：给定一个概率分布probs，$O(n)$时间建立一个查找table，通过查这个table可以在$O(1)$时间内产生服从该分布的一个样本；\n",
    "* 第一步：alias_setup函数的作用是给定一个概率分布，产生两个数组（即查找表），`J`和`q`，如：\n",
    "```python\n",
    "J, q = alias_setup([0.1, 0.2 ,0.2, 0.5])\n",
    "print(J,\"\\n\", q)\n",
    "[3 3 3 0] \n",
    "[0.4       0.8       0.8       0.9999999]\n",
    "```\n",
    "    * `J`数组的每一个位置都满足概率为1，其中索引`i`对应的值`J[i]`代表着索引`J[i]`处借了一部分概率给索引`i`，因为必须都为1;\n",
    "    * `q`数组中索引`i`处的值代表该去掉从其他索引处借来概率后，本来表示`i`这个位置的概率。\n",
    "\n",
    "* 第二步：alias_draw函数的作用是从该概率分布上取样一个样本，在$O(1)$时间内产生服从该分布的一个样本；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author：ZC\n",
    "class Alias:\n",
    "    def __init__(self, prolist):\n",
    "        self.pro = prolist\n",
    "        self.Initialization()\n",
    "    def Initialization(self):\n",
    "        self.N = len(self.pro)\n",
    "        self.alias, self.prob = [0] * self.N, [0] * self.N\n",
    "        small, large = deque(), deque()\n",
    "        self.pro *= self.N\n",
    "        for i in range(self.N):\n",
    "            if self.pro[i] < 1:\n",
    "                small.append(i)\n",
    "            else:\n",
    "                large.append(i)\n",
    "        while small and large:\n",
    "            l = small.popleft()\n",
    "            g = large.popleft()\n",
    "            self.prob[l] = self.pro[i]\n",
    "            self.alias[l] = g\n",
    "            self.pro[g] = self.pro[g] + self.pro[l] - 1\n",
    "        if self.pro[g] < 1:\n",
    "            small.append(g)\n",
    "        else:\n",
    "            large.append(g)\n",
    "        while large:\n",
    "            g = large.popleft()\n",
    "            self.prob[g] = 1\n",
    "        while small:\n",
    "            l = small.popleft()\n",
    "            self.prob[l] = 1\n",
    "    def Generation(self):\n",
    "        die = int(np.floor(np.random.random(1) * self.N))\n",
    "        coin = np.random.binomial(1,self.prob[die])\n",
    "        print(\"die Roll: %s coin Toss: %s\" %(die,coin))\n",
    "        if coin:\n",
    "            return die\n",
    "        else:\n",
    "            return self.alias[die]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die Roll: 2 coin Toss: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro = np.array([1/2,1/3,1/12,1/12],dtype=np.float32)\n",
    "pro # array([0.5       , 0.33333334, 0.08333334, 0.08333334], dtype=float32)\n",
    "sampling = Alias(pro)\n",
    "\n",
    "sampling.Generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy        as np\n",
    "import numpy.random as npr\n",
    "\n",
    "def alias_setup(probs):\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "    # Sort the data into the outcomes with probabilities\n",
    "    # that are larger and smaller than 1/K.\n",
    "    smaller = []\n",
    "    larger  = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    # Loop though and create little binary mixtures that\n",
    "    # appropriately allocate the larger outcomes over the\n",
    "    # overall uniform mixture.\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] - (1.0 - q[small])\n",
    "\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "def alias_draw(J, q):\n",
    "    K  = len(J)\n",
    "    # Draw from the overall uniform mixture.\n",
    "    kk = int(np.floor(npr.rand()*K))\n",
    "    # Draw from the binary mixture, either keeping the\n",
    "    # small one, or choosing the associated larger one.\n",
    "    if npr.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]\n",
    "\n",
    "K = 5\n",
    "N = 1000\n",
    "\n",
    "# Get a random probability vector.\n",
    "probs = npr.dirichlet(np.ones(K), 1).ravel()\n",
    "\n",
    "# Construct the table.\n",
    "J, q = alias_setup(probs)\n",
    "\n",
    "# Generate variates.\n",
    "X = np.zeros(N)\n",
    "for nn in range(N):\n",
    "    X[nn] = alias_draw(J, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0]), array([5.33333302, 2.66666698, 1.33333337, 1.33333337]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alias_setup(pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    '''\n",
    "    Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    for details\n",
    "    '''\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K, dtype=np.float32)\n",
    "    J = np.zeros(K, dtype=np.int32)\n",
    "\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "J, q = alias_setup([0.2, 0.2 ,0.2, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0] [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(J, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {4: 2008, 0: 2025, 1: 1948, 3: 1977, 2: 2042})"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "res = defaultdict(int)\n",
    "for _ in range(10000):\n",
    "    res[alias_draw(J,q)] += 1\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_draw(J, q):\n",
    "    K  = len(J)\n",
    "\n",
    "    # Draw from the overall uniform mixture.\n",
    "    kk = int(np.floor(npr.rand()*K))\n",
    "\n",
    "    # Draw from the binary mixture, either keeping the\n",
    "    # small one, or choosing the associated larger one.\n",
    "    if npr.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "N = 10\n",
    "\n",
    "# Get a random probability vector.\n",
    "probs = npr.dirichlet(np.ones(K), 1).ravel()\n",
    "\n",
    "# Construct the table.\n",
    "J, q = alias_setup(probs)\n",
    "\n",
    "# Generate variates.\n",
    "X = np.zeros(N)\n",
    "for nn in range(N):\n",
    "    X[nn] = alias_draw(J, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3., 1., 1., 2., 2., 1., 4., 3., 1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.RandomState?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version_info > (3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/zhengchu/Documents/blogs/personal/RUGCN/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset | Classes |  Features | Nodes | Edges\n",
    "\n",
    "> CORA    | 7        | 1433     | 2,708  | 5,278 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "    All objects above must be saved using python pickle module.\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"/Users/zhengchu/Documents/blogs/personal/RUGCN/data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"/Users/zhengchu/Documents/blogs/personal/RUGCN/data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return sparse_to_tuple(t_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'citeseer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "objects = []\n",
    "for i in range(len(names)):\n",
    "    with open(\"/Users/zhengchu/Documents/blogs/personal/RUGCN/data/ind.{}.{}\".format(dataset, names[i]), 'rb') as f:\n",
    "        if sys.version_info > (3, 0):\n",
    "            objects.append(pkl.load(f, encoding='latin1'))\n",
    "        else:\n",
    "            objects.append(pkl.load(f))\n",
    "\n",
    "x, y, tx, ty, allx, ally, graph = tuple(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "    All objects above must be saved using python pickle module.\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 1433)\n",
      "(140, 7)\n",
      "(1000, 1433)\n",
      "(1000, 7)\n",
      "(1708, 1433)\n",
      "(1708, 7)\n"
     ]
    }
   ],
   "source": [
    "for it in [x, y, tx, ty, allx, ally]:\n",
    "    print(it.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2706, 2707]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(graph.keys())[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx_reorder = parse_index_file(\"/Users/zhengchu/Documents/blogs/personal/RUGCN/data/ind.{}.test.index\".format(dataset))\n",
    "test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "if dataset == 'citeseer':\n",
    "    # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "    # Find isolated nodes, add them as zero-vecs into the right position\n",
    "    test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "    tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "    tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "    tx = tx_extended\n",
    "    ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "    ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "    ty = ty_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718,\n",
       "       1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729,\n",
       "       1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740,\n",
       "       1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751,\n",
       "       1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762,\n",
       "       1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773,\n",
       "       1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784,\n",
       "       1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795,\n",
       "       1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806,\n",
       "       1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817,\n",
       "       1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828,\n",
       "       1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839,\n",
       "       1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850,\n",
       "       1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861,\n",
       "       1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872,\n",
       "       1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883,\n",
       "       1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894,\n",
       "       1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905,\n",
       "       1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916,\n",
       "       1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927,\n",
       "       1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938,\n",
       "       1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949,\n",
       "       1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960,\n",
       "       1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971,\n",
       "       1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982,\n",
       "       1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993,\n",
       "       1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004,\n",
       "       2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015,\n",
       "       2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026,\n",
       "       2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037,\n",
       "       2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048,\n",
       "       2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059,\n",
       "       2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070,\n",
       "       2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081,\n",
       "       2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092,\n",
       "       2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103,\n",
       "       2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114,\n",
       "       2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125,\n",
       "       2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136,\n",
       "       2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147,\n",
       "       2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158,\n",
       "       2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169,\n",
       "       2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180,\n",
       "       2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191,\n",
       "       2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202,\n",
       "       2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213,\n",
       "       2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224,\n",
       "       2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235,\n",
       "       2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246,\n",
       "       2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257,\n",
       "       2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268,\n",
       "       2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279,\n",
       "       2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290,\n",
       "       2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301,\n",
       "       2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312,\n",
       "       2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323,\n",
       "       2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334,\n",
       "       2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345,\n",
       "       2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356,\n",
       "       2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367,\n",
       "       2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378,\n",
       "       2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389,\n",
       "       2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400,\n",
       "       2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411,\n",
       "       2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422,\n",
       "       2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433,\n",
       "       2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444,\n",
       "       2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455,\n",
       "       2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466,\n",
       "       2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477,\n",
       "       2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488,\n",
       "       2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499,\n",
       "       2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510,\n",
       "       2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521,\n",
       "       2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532,\n",
       "       2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543,\n",
       "       2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554,\n",
       "       2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565,\n",
       "       2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576,\n",
       "       2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587,\n",
       "       2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598,\n",
       "       2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609,\n",
       "       2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620,\n",
       "       2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631,\n",
       "       2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642,\n",
       "       2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653,\n",
       "       2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664,\n",
       "       2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675,\n",
       "       2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686,\n",
       "       2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697,\n",
       "       2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split数据测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import sys\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "\n",
    "\n",
    "def _eliminate_self_loops(A):\n",
    "    \"\"\"Remove self-loops from the adjacency matrix.\"\"\"\n",
    "    A = A.tolil()\n",
    "    A.setdiag(0)\n",
    "    A = A.tocsr()\n",
    "    A.eliminate_zeros()\n",
    "    return A\n",
    "\n",
    "def eliminate_self_loops(G):\n",
    "    G.adj_matrix = _eliminate_self_loops(G.adj_matrix)\n",
    "    return G\n",
    "\n",
    "\n",
    "def largest_connected_components(sparse_graph, n_components=1):\n",
    "    \"\"\"Select the largest connected components in the graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_graph : SparseGraph\n",
    "        Input graph.\n",
    "    n_components : int, default 1\n",
    "        Number of largest connected components to keep.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sparse_graph : SparseGraph\n",
    "        Subgraph of the input graph where only the nodes in largest n_components are kept.\n",
    "\n",
    "    \"\"\"\n",
    "    _, component_indices = sp.csgraph.connected_components(sparse_graph.adj_matrix)\n",
    "    component_sizes = np.bincount(component_indices)\n",
    "    components_to_keep = np.argsort(component_sizes)[::-1][:n_components]  # reverse order to sort descending\n",
    "    nodes_to_keep = [\n",
    "        idx for (idx, component) in enumerate(component_indices) if component in components_to_keep\n",
    "    ]\n",
    "    return create_subgraph(sparse_graph, nodes_to_keep=nodes_to_keep)\n",
    "\n",
    "def get_train_val_test_split(random_state,\n",
    "                             labels,\n",
    "                             train_examples_per_class=None, val_examples_per_class=None,\n",
    "                             test_examples_per_class=None,\n",
    "                             train_size=None, val_size=None, test_size=None):\n",
    "    num_samples, num_classes = labels.shape\n",
    "    remaining_indices = list(range(num_samples))\n",
    "\n",
    "    if train_examples_per_class is not None:\n",
    "        train_indices = sample_per_class(random_state, labels, train_examples_per_class)\n",
    "    else:\n",
    "        # select train examples with no respect to class distribution\n",
    "        train_indices = random_state.choice(remaining_indices, train_size, replace=False)\n",
    "\n",
    "    if val_examples_per_class is not None:\n",
    "        val_indices = sample_per_class(random_state, labels, val_examples_per_class, forbidden_indices=train_indices)\n",
    "    else:\n",
    "        remaining_indices = np.setdiff1d(remaining_indices, train_indices)\n",
    "        val_indices = random_state.choice(remaining_indices, val_size, replace=False)\n",
    "\n",
    "    forbidden_indices = np.concatenate((train_indices, val_indices))\n",
    "    if test_examples_per_class is not None:\n",
    "        test_indices = sample_per_class(random_state, labels, test_examples_per_class,\n",
    "                                        forbidden_indices=forbidden_indices)\n",
    "    elif test_size is not None:\n",
    "        remaining_indices = np.setdiff1d(remaining_indices, forbidden_indices)\n",
    "        test_indices = random_state.choice(remaining_indices, test_size, replace=False)\n",
    "    else:\n",
    "        test_indices = np.setdiff1d(remaining_indices, forbidden_indices)\n",
    "\n",
    "    # assert that there are no duplicates in sets\n",
    "    assert len(set(train_indices)) == len(train_indices)\n",
    "    assert len(set(val_indices)) == len(val_indices)\n",
    "    assert len(set(test_indices)) == len(test_indices)\n",
    "    # assert sets are mutually exclusive\n",
    "    assert len(set(train_indices) - set(val_indices)) == len(set(train_indices))\n",
    "    assert len(set(train_indices) - set(test_indices)) == len(set(train_indices))\n",
    "    assert len(set(val_indices) - set(test_indices)) == len(set(val_indices))\n",
    "    if test_size is None and test_examples_per_class is None:\n",
    "        # all indices must be part of the split\n",
    "        assert len(np.concatenate((train_indices, val_indices, test_indices))) == num_samples\n",
    "\n",
    "    if train_examples_per_class is not None:\n",
    "        train_labels = labels[train_indices, :]\n",
    "        train_sum = np.sum(train_labels, axis=0)\n",
    "        # assert all classes have equal cardinality\n",
    "        assert np.unique(train_sum).size == 1\n",
    "\n",
    "    if val_examples_per_class is not None:\n",
    "        val_labels = labels[val_indices, :]\n",
    "        val_sum = np.sum(val_labels, axis=0)\n",
    "        # assert all classes have equal cardinality\n",
    "        assert np.unique(val_sum).size == 1\n",
    "\n",
    "    if test_examples_per_class is not None:\n",
    "        test_labels = labels[test_indices, :]\n",
    "        test_sum = np.sum(test_labels, axis=0)\n",
    "        # assert all classes have equal cardinality\n",
    "        assert np.unique(test_sum).size == 1\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "\n",
    "def sample_per_class(random_state, labels, num_examples_per_class, forbidden_indices=None):\n",
    "    num_samples, num_classes = labels.shape\n",
    "    sample_indices_per_class = {index: [] for index in range(num_classes)}\n",
    "\n",
    "    # get indices sorted by class\n",
    "    for class_index in range(num_classes):\n",
    "        for sample_index in range(num_samples):\n",
    "            if labels[sample_index, class_index] > 0.0:\n",
    "                if forbidden_indices is None or sample_index not in forbidden_indices:\n",
    "                    sample_indices_per_class[class_index].append(sample_index)\n",
    "\n",
    "    # get specified number of indices for each class\n",
    "    return np.concatenate(\n",
    "        [random_state.choice(sample_indices_per_class[class_index], num_examples_per_class, replace=False)\n",
    "         for class_index in range(len(sample_indices_per_class))\n",
    "         ])\n",
    "\n",
    "def is_binary_bag_of_words(features):\n",
    "    features_coo = features.tocoo()\n",
    "    return all(single_entry == 1.0 for _, _, single_entry in zip(features_coo.row, features_coo.col, features_coo.data))\n",
    "\n",
    "def to_binary_bag_of_words(features):\n",
    "    \"\"\"Converts TF/IDF features to binary bag-of-words features.\"\"\"\n",
    "    features_copy = features.tocsr()\n",
    "    features_copy.data[:] = 1.0\n",
    "    return features_copy\n",
    "\n",
    "def binarize_labels(labels, sparse_output=False, return_classes=False):\n",
    "    \"\"\"Convert labels vector to a binary label matrix.\n",
    "\n",
    "    In the default single-label case, labels look like\n",
    "    labels = [y1, y2, y3, ...].\n",
    "    Also supports the multi-label format.\n",
    "    In this case, labels should look something like\n",
    "    labels = [[y11, y12], [y21, y22, y23], [y31], ...].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : array-like, shape [num_samples]\n",
    "        Array of node labels in categorical single- or multi-label format.\n",
    "    sparse_output : bool, default False\n",
    "        Whether return the label_matrix in CSR format.\n",
    "    return_classes : bool, default False\n",
    "        Whether return the classes corresponding to the columns of the label matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label_matrix : np.ndarray or sp.csr_matrix, shape [num_samples, num_classes]\n",
    "        Binary matrix of class labels.\n",
    "        num_classes = number of unique values in \"labels\" array.\n",
    "        label_matrix[i, k] = 1 <=> node i belongs to class k.\n",
    "    classes : np.array, shape [num_classes], optional\n",
    "        Classes that correspond to each column of the label_matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    if hasattr(labels[0], '__iter__'):  # labels[0] is iterable <=> multilabel format\n",
    "        binarizer = MultiLabelBinarizer(sparse_output=sparse_output)\n",
    "    else:\n",
    "        binarizer = LabelBinarizer(sparse_output=sparse_output)\n",
    "    label_matrix = binarizer.fit_transform(labels).astype(np.float32)\n",
    "    return (label_matrix, binarizer.classes_) if return_classes else label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGraph:\n",
    "    \"\"\"Attributed labeled graph stored in sparse matrix form.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_matrix, attr_matrix=None, labels=None,\n",
    "                 node_names=None, attr_names=None, class_names=None, metadata=None):\n",
    "        \"\"\"Create an attributed graph.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adj_matrix : sp.csr_matrix, shape [num_nodes, num_nodes]\n",
    "            Adjacency matrix in CSR format.\n",
    "        attr_matrix : sp.csr_matrix or np.ndarray, shape [num_nodes, num_attr], optional\n",
    "            Attribute matrix in CSR or numpy format.\n",
    "        labels : np.ndarray, shape [num_nodes], optional\n",
    "            Array, where each entry represents respective node's label(s).\n",
    "        node_names : np.ndarray, shape [num_nodes], optional\n",
    "            Names of nodes (as strings).\n",
    "        attr_names : np.ndarray, shape [num_attr]\n",
    "            Names of the attributes (as strings).\n",
    "        class_names : np.ndarray, shape [num_classes], optional\n",
    "            Names of the class labels (as strings).\n",
    "        metadata : object\n",
    "            Additional metadata such as text.\n",
    "\n",
    "        \"\"\"\n",
    "        # Make sure that the dimensions of matrices / arrays all agree\n",
    "        if sp.isspmatrix(adj_matrix):\n",
    "            adj_matrix = adj_matrix.tocsr().astype(np.float32)\n",
    "        else:\n",
    "            raise ValueError(\"Adjacency matrix must be in sparse format (got {0} instead)\"\n",
    "                             .format(type(adj_matrix)))\n",
    "\n",
    "        if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "            raise ValueError(\"Dimensions of the adjacency matrix don't agree\")\n",
    "\n",
    "        if attr_matrix is not None:\n",
    "            if sp.isspmatrix(attr_matrix):\n",
    "                attr_matrix = attr_matrix.tocsr().astype(np.float32)\n",
    "            elif isinstance(attr_matrix, np.ndarray):\n",
    "                attr_matrix = attr_matrix.astype(np.float32)\n",
    "            else:\n",
    "                raise ValueError(\"Attribute matrix must be a sp.spmatrix or a np.ndarray (got {0} instead)\"\n",
    "                                 .format(type(attr_matrix)))\n",
    "\n",
    "            if attr_matrix.shape[0] != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency and attribute matrices don't agree\")\n",
    "\n",
    "        if labels is not None:\n",
    "            if labels.shape[0] != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency matrix and the label vector don't agree\")\n",
    "\n",
    "        if node_names is not None:\n",
    "            if len(node_names) != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency matrix and the node names don't agree\")\n",
    "\n",
    "        if attr_names is not None:\n",
    "            if len(attr_names) != attr_matrix.shape[1]:\n",
    "                raise ValueError(\"Dimensions of the attribute matrix and the attribute names don't agree\")\n",
    "\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.attr_matrix = attr_matrix\n",
    "        self.labels = labels\n",
    "        self.node_names = node_names\n",
    "        self.attr_names = attr_names\n",
    "        self.class_names = class_names\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def num_nodes(self):\n",
    "        \"\"\"Get the number of nodes in the graph.\"\"\"\n",
    "        return self.adj_matrix.shape[0]\n",
    "\n",
    "    def num_edges(self):\n",
    "        \"\"\"Get the number of edges in the graph.\n",
    "\n",
    "        For undirected graphs, (i, j) and (j, i) are counted as single edge.\n",
    "        \"\"\"\n",
    "        if self.is_directed():\n",
    "            return int(self.adj_matrix.nnz)\n",
    "        else:\n",
    "            return int(self.adj_matrix.nnz / 2)\n",
    "\n",
    "    def get_neighbors(self, idx):\n",
    "        \"\"\"Get the indices of neighbors of a given node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the node whose neighbors are of interest.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.adj_matrix[idx].indices\n",
    "\n",
    "    def is_directed(self):\n",
    "        \"\"\"Check if the graph is directed (adjacency matrix is not symmetric).\"\"\"\n",
    "        return (self.adj_matrix != self.adj_matrix.T).sum() != 0\n",
    "\n",
    "    def to_undirected(self):\n",
    "        \"\"\"Convert to an undirected graph (make adjacency matrix symmetric).\"\"\"\n",
    "        if self.is_weighted():\n",
    "            raise ValueError(\"Convert to unweighted graph first.\")\n",
    "        else:\n",
    "            self.adj_matrix = self.adj_matrix + self.adj_matrix.T\n",
    "            self.adj_matrix[self.adj_matrix != 0] = 1\n",
    "        return self\n",
    "\n",
    "    def is_weighted(self):\n",
    "        \"\"\"Check if the graph is weighted (edge weights other than 1).\"\"\"\n",
    "        return np.any(np.unique(self.adj_matrix[self.adj_matrix != 0].A1) != 1)\n",
    "\n",
    "    def to_unweighted(self):\n",
    "        \"\"\"Convert to an unweighted graph (set all edge weights to 1).\"\"\"\n",
    "        self.adj_matrix.data = np.ones_like(self.adj_matrix.data)\n",
    "        return self\n",
    "\n",
    "    # Quality of life (shortcuts)\n",
    "    def standardize(self):\n",
    "        \"\"\"Select the LCC of the unweighted/undirected/no-self-loop graph.\n",
    "\n",
    "        All changes are done inplace.\n",
    "\n",
    "        \"\"\"\n",
    "        G = self.to_unweighted().to_undirected()\n",
    "        G = eliminate_self_loops(G)\n",
    "        G = largest_connected_components(G, 1)\n",
    "        print(\"is Graph un-symmetric after standardize: \", G.is_directed())\n",
    "        # G = G.to_unweighted().to_undirected()\n",
    "        print(\"is Graph directed after standardize: \", G.is_directed())\n",
    "        return G\n",
    "\n",
    "    def unpack(self):\n",
    "        \"\"\"Return the (A, X, z) triplet.\"\"\"\n",
    "        return self.adj_matrix, self.attr_matrix, self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_to_sparse_graph(file_name):\n",
    "    \"\"\"Load a SparseGraph from a Numpy binary file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Name of the file to load.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sparse_graph : SparseGraph\n",
    "        Graph in sparse matrix format.\n",
    "\n",
    "    \"\"\"\n",
    "    file_name = \"/Users/zhengchu/Documents/blogs/personal/RUGCN/data/{}\".format(file_name)\n",
    "    with np.load(file_name,  allow_pickle=True) as loader:\n",
    "        loader = dict(loader)\n",
    "        adj_matrix = sp.csr_matrix((loader['adj_data'], loader['adj_indices'], loader['adj_indptr']),\n",
    "                                   shape=loader['adj_shape'])\n",
    "\n",
    "        if 'attr_data' in loader:\n",
    "            # Attributes are stored as a sparse CSR matrix\n",
    "            attr_matrix = sp.csr_matrix((loader['attr_data'], loader['attr_indices'], loader['attr_indptr']),\n",
    "                                        shape=loader['attr_shape'])\n",
    "        elif 'attr_matrix' in loader:\n",
    "            # Attributes are stored as a (dense) np.ndarray\n",
    "            attr_matrix = loader['attr_matrix']\n",
    "        else:\n",
    "            attr_matrix = None\n",
    "\n",
    "        if 'labels_data' in loader:\n",
    "            # Labels are stored as a CSR matrix\n",
    "            labels = sp.csr_matrix((loader['labels_data'], loader['labels_indices'], loader['labels_indptr']),\n",
    "                                   shape=loader['labels_shape'])\n",
    "        elif 'labels' in loader:\n",
    "            # Labels are stored as a numpy array\n",
    "            labels = loader['labels']\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        node_names = loader.get('node_names')\n",
    "        attr_names = loader.get('attr_names')\n",
    "        class_names = loader.get('class_names')\n",
    "        metadata = loader.get('metadata')\n",
    "\n",
    "    return SparseGraph(adj_matrix, attr_matrix, labels, node_names, attr_names, class_names, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_subgraph(sparse_graph, _sentinel=None, nodes_to_remove=None, nodes_to_keep=None):\n",
    "    \"\"\"Create a graph with the specified subset of nodes.\n",
    "\n",
    "    Exactly one of (nodes_to_remove, nodes_to_keep) should be provided, while the other stays None.\n",
    "    Note that to avoid confusion, it is required to pass node indices as named arguments to this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_graph : SparseGraph\n",
    "        Input graph.\n",
    "    _sentinel : None\n",
    "        Internal, to prevent passing positional arguments. Do not use.\n",
    "    nodes_to_remove : array-like of int\n",
    "        Indices of nodes that have to removed.\n",
    "    nodes_to_keep : array-like of int\n",
    "        Indices of nodes that have to be kept.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sparse_graph : SparseGraph\n",
    "        Graph with specified nodes removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check that arguments are passed correctly\n",
    "    if _sentinel is not None:\n",
    "        raise ValueError(\"Only call `create_subgraph` with named arguments',\"\n",
    "                         \" (nodes_to_remove=...) or (nodes_to_keep=...)\")\n",
    "    if nodes_to_remove is None and nodes_to_keep is None:\n",
    "        raise ValueError(\"Either nodes_to_remove or nodes_to_keep must be provided.\")\n",
    "    elif nodes_to_remove is not None and nodes_to_keep is not None:\n",
    "        raise ValueError(\"Only one of nodes_to_remove or nodes_to_keep must be provided.\")\n",
    "    elif nodes_to_remove is not None:\n",
    "        nodes_to_keep = [i for i in range(sparse_graph.num_nodes()) if i not in nodes_to_remove]\n",
    "    elif nodes_to_keep is not None:\n",
    "        nodes_to_keep = sorted(nodes_to_keep)\n",
    "    else:\n",
    "        raise RuntimeError(\"This should never happen.\")\n",
    "\n",
    "    sparse_graph.adj_matrix = sparse_graph.adj_matrix[nodes_to_keep][:, nodes_to_keep]\n",
    "    if sparse_graph.attr_matrix is not None:\n",
    "        sparse_graph.attr_matrix = sparse_graph.attr_matrix[nodes_to_keep]\n",
    "    if sparse_graph.labels is not None:\n",
    "        sparse_graph.labels = sparse_graph.labels[nodes_to_keep]\n",
    "    if sparse_graph.node_names is not None:\n",
    "        sparse_graph.node_names = sparse_graph.node_names[nodes_to_keep]\n",
    "    return sparse_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_graph = load_npz_to_sparse_graph(dataset + '.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is Graph un-symmetric after standardize:  False\n",
      "is Graph directed after standardize:  False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1、先标准化图，即构造最大连通分量，去掉孤立节点\n",
    "2、去掉自环\n",
    "3、返回1、2步处理后得到的图的邻接矩阵adj，特征features，对应节点的标签labels\n",
    "\n",
    "4、特征与标签进行特殊处理；\n",
    "5、采样训练节点、验证节点和测试节点；\n",
    "\"\"\"\n",
    "dataset_graph = dataset_graph.standardize()\n",
    "dataset_graph = eliminate_self_loops(dataset_graph)\n",
    "adj, features, labels = dataset_graph.unpack()\n",
    "\n",
    "labels = binarize_labels(labels)\n",
    "if not is_binary_bag_of_words(features):\n",
    "    print(\"Converting features of dataset {} to binary bag-of-words representation.\".format(dataset_str[:-4]))\n",
    "    features = to_binary_bag_of_words(features)\n",
    "    \n",
    "RandomState = np.random.RandomState(3)\n",
    "idx_train, idx_val, idx_test = get_train_val_test_split(RandomState, labels, train_examples_per_class=20,\n",
    "                                                        val_examples_per_class=30,\n",
    "                                                        test_examples_per_class=None,\n",
    "                                                        train_size=None, val_size=None, test_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj shape: (2110, 2110), features shape: (2110, 3703), labels shape: (2110, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"adj shape: {adj.shape}, features shape: {features.shape}, labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 120, val_size: 180, test_size: 1810\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_size: {idx_train.size}, val_size: {idx_val.size}, test_size: {idx_test.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adj是否对称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adj != adj.T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_g, col_g = adj.nonzero()\n",
    "index = list(zip(raw_g, col_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj[(1, 4)].astype(np.int32) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [ 488  712  734  982 1785 1862]\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "cur_row = 4\n",
    "_, cur_col = adj[cur_row,:].nonzero()\n",
    "print(cur_row, cur_col)\n",
    "print(np.ones(len(cur_col)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class Walker:\n",
    "    def __init__(self, adj, features, idx_train, idx_val, idx_test, num_walks, walk_length, p = 1.0, q = 1.0):\n",
    "        self.adj = adj\n",
    "        self.features = features\n",
    "        self.idx_train = idx_train\n",
    "        self.idx_val = idx_val\n",
    "        self.idx_test = idx_test\n",
    "        self.num_walks = num_walks\n",
    "        self.walk_length = walk_length\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.process_probs().simulate_walk(self.num_walks, walk_length)\n",
    "    \n",
    "    def simulate_walk(self, num_walks, walk_length):\n",
    "        walks = defaultdict(list)\n",
    "        for i in tqdm(range(num_walks)):\n",
    "            print(f\"Walk iteration: {i + 1}\")\n",
    "            index = np.concatenate([self.idx_val, self.idx_test])\n",
    "            for idx in index:\n",
    "                res = self.node2vec_walk(walk_length, idx)\n",
    "                walks[idx] += res[1:]\n",
    "        self.walks = walks\n",
    "            \n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "        walk = [start_node]\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            _, neighs = self.adj[cur, :].nonzero()\n",
    "            if len(neighs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    res = neighs[\n",
    "                            self.alias_draw(self.alias_nodes[cur][0]\n",
    "                                            , self.alias_nodes[cur][1])]\n",
    "                    if res in self.idx_train:\n",
    "                        break\n",
    "                    else:\n",
    "                        walk.append(res)\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    edge = (prev, cur)\n",
    "                    res = neighs[\n",
    "                            self.alias_draw(self.alias_edges[edge][0],\n",
    "                                            self.alias_edges[edge][1])]\n",
    "                    if res in self.idx_train:\n",
    "                        break\n",
    "                    else:\n",
    "                        walk.append(res)\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "        \n",
    "    def process_probs(self):\n",
    "        adj = self.adj\n",
    "        alias_nodes = {}\n",
    "        size = adj.shape[0]\n",
    "        for cur_row in range(size):\n",
    "            _, cur_col = adj[cur_row,:].nonzero()\n",
    "            unnormalized_probs = np.ones(len(cur_col))\n",
    "            deno = unnormalized_probs.sum()\n",
    "            normalized_probs = unnormalized_probs / deno\n",
    "            alias_nodes[cur_row] = self.alias_setup(normalized_probs)\n",
    "        \n",
    "        alias_edges = {}\n",
    "        raw_g, col_g = adj.nonzero()\n",
    "        for edge in zip(raw_g, col_g):\n",
    "            alias_edges[edge] = self.get_alias_edges(edge[0], edge[1])\n",
    "        self.alias_nodes = alias_nodes\n",
    "        self.alias_edges = alias_edges\n",
    "        return self\n",
    "        \n",
    "    def get_alias_edges(self, src, dst):\n",
    "        unnormalized_probs = []\n",
    "        _, neighs = self.adj[dst,:].nonzero()\n",
    "        for neigh in neighs:\n",
    "            if neigh == src:\n",
    "                unnormalized_probs.append(1.0 / self.p)\n",
    "            elif adj[(src, neigh)] or adj[(neigh,src)]:\n",
    "                unnormalized_probs.append(1.0)\n",
    "            else:\n",
    "                unnormalized_probs.append(1.0 / self.q)\n",
    "            deno = sum(unnormalized_probs)\n",
    "            normalized_probs = [float(prob) / deno for prob in unnormalized_probs]\n",
    "        return self.alias_setup(normalized_probs)\n",
    "            \n",
    "            \n",
    "    def alias_setup(self, probs):\n",
    "        '''\n",
    "        Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "        Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "        for details\n",
    "        '''\n",
    "        K = len(probs)\n",
    "        q = np.zeros(K, dtype=np.float32)\n",
    "        J = np.zeros(K, dtype=np.int32)\n",
    "\n",
    "        smaller = []\n",
    "        larger = []\n",
    "        for kk, prob in enumerate(probs):\n",
    "            q[kk] = K*prob\n",
    "            if q[kk] < 1.0:\n",
    "                smaller.append(kk)\n",
    "            else:\n",
    "                larger.append(kk)\n",
    "\n",
    "        while len(smaller) > 0 and len(larger) > 0:\n",
    "            small = smaller.pop()\n",
    "            large = larger.pop()\n",
    "\n",
    "            J[small] = large\n",
    "            q[large] = q[large] + q[small] - 1.0\n",
    "            if q[large] < 1.0:\n",
    "                smaller.append(large)\n",
    "            else:\n",
    "                larger.append(large)\n",
    "\n",
    "        return J, q\n",
    "    \n",
    "    def alias_draw(self, J, q):\n",
    "        K  = len(J)\n",
    "\n",
    "        # Draw from the overall uniform mixture.\n",
    "        kk = int(np.floor(npr.rand()*K))\n",
    "\n",
    "        # Draw from the binary mixture, either keeping the\n",
    "        # small one, or choosing the associated larger one.\n",
    "        if npr.rand() < q[kk]:\n",
    "            return kk\n",
    "        else:\n",
    "            return J[kk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:02<00:26,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:05<00:23,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:08<00:20,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:11<00:16,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:14<00:13,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:16<00:10,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:19<00:07,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:22<00:05,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:25<00:02,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "walker = Walker(adj, features, idx_train, idx_val, idx_test, num_walks=10, walk_length=10, p=1.0, q=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = walker.walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_probs = []\n",
    "edges = []\n",
    "for st_node, dest_nodes in walks.items():\n",
    "    sigle_node_unnormalized_prob = []\n",
    "    for dest_node, ct in Counter(dest_nodes).items():\n",
    "        sigle_node_unnormalized_prob.append(ct)\n",
    "        edges.append([st_node, dest_node])\n",
    "    sigle_node_unnormalized_prob = np.array(sigle_node_unnormalized_prob)\n",
    "    unnormalized_probs.append(sigle_node_unnormalized_prob / sigle_node_unnormalized_prob.sum())\n",
    "unnormalized_probs = np.concatenate(unnormalized_probs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50200, 50200)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unnormalized_probs), len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_probs = []\n",
    "edges = []\n",
    "for dest_node in dest_nodes:\n",
    "    sigle_node_unnormalized_prob = []\n",
    "    for st_node, (k, v) in zip(st_nodes, Counter(dest_node).items()):\n",
    "        sigle_node_unnormalized_prob.append(v)\n",
    "        edges.append([st_node, k])\n",
    "    sigle_node_unnormalized_prob = np.array(sigle_node_unnormalized_prob)\n",
    "    sigle_node_unnormalized_prob = sigle_node_unnormalized_prob / sigle_node_unnormalized_prob.sum()\n",
    "    unnormalized_probs.append(sigle_node_unnormalized_prob)\n",
    "\n",
    "unnormalized_probs = np.concatenate(unnormalized_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60682, 60682)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unnormalized_probs), len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __dataset_generator(self, idx_train, idx_val, idx_test, neighbor):\n",
    "        print(\"neighbor size is : \", neighbor)\n",
    "        idx_train = idx_train.tolist()\n",
    "        idx_val = idx_val.tolist()\n",
    "        idx_test = idx_test.tolist()\n",
    "\n",
    "        # 未标签节点与未标签节点间的边: 验证集节点\n",
    "        uuvaledges = []\n",
    "        for item in idx_val:\n",
    "            neibors = list(self.G.neighbors(item))\n",
    "            if len(neibors) != 0 and len(neibors) <= neighbor:\n",
    "                for neibor in neibors:\n",
    "                    if neibor not in idx_train:\n",
    "                        uuvaledges.append([item, neibor])\n",
    "        # 未标签节点与未标签节点间的边: 测试集节点\n",
    "        uutestedges = []\n",
    "        for item in idx_test:\n",
    "            neibors = list(self.G.neighbors(item))\n",
    "            if len(neibors) != 0 and len(neibors) <= neighbor:\n",
    "                for neibor in neibors:\n",
    "                    if neibor not in idx_train:\n",
    "                        uutestedges.append([item, neibor])\n",
    "        self.train_triples = torch.LongTensor(np.concatenate([uuvaledges, uutestedges]))\n",
    "        self.index = self.train_triples.transpose(0, 1).contiguous()\n",
    "        self.src = torch.as_tensor(list(set(self.index[0, :].tolist())))\n",
    "\n",
    "        # self.train_triples = self.train_triples[torch.randperm(len(self.train_triples))]\n",
    "\n",
    "        print(\"training edges size :\", self.train_triples.size(), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 546  994 1231 1246 1530]\n",
      "[1016 1823]\n",
      "[  14  247  618  740 1390 1910]\n",
      "[665]\n",
      "[ 156 1877]\n",
      "[1156]\n",
      "[ 400  409  574  593 1638 1639]\n",
      "[   4  103 1324]\n",
      "[ 227  563  625  641 1155 1953]\n",
      "[1882]\n",
      "[ 168  278 1204 1565 1566 1784]\n",
      "[ 603  615 1656]\n",
      "[2028 2029 2030]\n",
      "[1626]\n",
      "[ 273  855 1063 1731]\n",
      "[ 14 175 247]\n",
      "[ 368 1739]\n",
      "[1252]\n",
      "[1694]\n",
      "[1015 1016 1705]\n",
      "[1750]\n",
      "[1348 1675 1687]\n",
      "[ 423  757 1414 1438 1749]\n",
      "[1791]\n",
      "[1010]\n",
      "[1348]\n",
      "[475 860]\n",
      "[ 471 1489]\n",
      "[  24  401 1710 1969]\n",
      "[ 790 1345]\n",
      "[1586]\n",
      "[ 570 1209 1415]\n",
      "[  47 1437]\n",
      "[1055 1492]\n",
      "[ 481 1356 1437]\n",
      "[1492 2108]\n",
      "[1178]\n",
      "[552]\n",
      "[ 356 1251]\n",
      "[1221]\n",
      "[97]\n",
      "[801]\n",
      "[ 238  952 1575 2097 2099]\n",
      "[  36  147  607 1152 1632]\n",
      "[ 949 1923]\n",
      "[  32   87  275  643 1088 1089]\n",
      "[2094 2099]\n",
      "[460]\n",
      "[808 809 978]\n",
      "[610]\n",
      "[ 380  382  735  736  737  738 1158]\n",
      "[1218 1778]\n",
      "[ 121 1250 1511]\n",
      "[211]\n",
      "[1500]\n",
      "[ 173 1171 1617 1618]\n",
      "[871]\n",
      "[  58 1978]\n",
      "[ 169  389  400  964 1108 1154 1233 1404 1802]\n",
      "[75]\n",
      "[148 699]\n",
      "[ 623 1498]\n",
      "[ 794  846 1084 1868 1938]\n",
      "[2003]\n",
      "[  23 1451 1718 1839]\n",
      "[ 182  977 1189 1691 1843]\n",
      "[1039 1483]\n",
      "[1497]\n",
      "[1266 1267 1659]\n",
      "[ 196  503  810 1498 1557 1558]\n",
      "[ 411 1279]\n",
      "[  37  458  823 1277 1950]\n",
      "[1375]\n",
      "[ 319 1480]\n",
      "[1125 1694]\n",
      "[1177 1473]\n",
      "[  10 1130 2054]\n",
      "[ 312 1131 1846 1959]\n",
      "[1694]\n",
      "[ 23 844]\n",
      "[ 425  655 1003 1289]\n",
      "[ 470 1874 2009]\n",
      "[   2 1357 1992]\n",
      "[  22 1739 2025]\n",
      "[ 357 1366 1464 1874]\n",
      "[ 156  654 1345]\n",
      "[1270 1471]\n",
      "[  66  668 1001]\n",
      "[241 472 640 655 832 833 862 938 948]\n",
      "[1568]\n",
      "[ 938 1049 1454]\n",
      "[1159 1699 1894 1952]\n",
      "[ 598 1397 1509 1510 1602 1894]\n",
      "[ 144  224  718  746  862  938 1021 1271]\n",
      "[ 452  655  862 1289 1453]\n",
      "[ 151  266  441  655 1289 1481 1554 2083 2084 2085]\n",
      "[  89   98  157  218  452  528  619  640  817  862  938  983  999 1001\n",
      " 1019 1079 1144 1193 1194 1196 1428 1453 1454 1455 1554 1859 1894]\n",
      "[631 813]\n",
      "[655 862]\n",
      "[ 84 237 318]\n",
      "[ 406  923  924 1576]\n",
      "[645]\n",
      "[ 90 436]\n",
      "[1443]\n",
      "[   5   56  650  688 1085]\n",
      "[ 231 1786 1790]\n",
      "[ 203 1313 1536]\n",
      "[1808]\n",
      "[ 286  815 1429 1607]\n",
      "[ 241  403  584  614  805  950 1001 1452 1867]\n",
      "[ 322  455 1406 1725]\n",
      "[1149 1733 1788 1790]\n",
      "[ 427 1091 2020]\n",
      "[ 688 1074 1191 1333 1337 1338 1339]\n",
      "[ 110 1155 1798]\n",
      "[1543]\n",
      "[   5   56  652 1227 1368]\n",
      "[ 220  343  345  425  598 1003 1127 1428 1429 1430 1662 2073]\n",
      "[1327 1891]\n",
      "[  55 1309]\n"
     ]
    }
   ],
   "source": [
    "for idx_tr in idx_train:\n",
    "    print(adj[idx_tr].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomState = np.random.RandomState(3)\n",
    "idx_train, idx_val, idx_test = get_train_val_test_split(RandomState, labels, train_examples_per_class=20,\n",
    "                                                        val_examples_per_class=30,\n",
    "                                                        test_examples_per_class=None,\n",
    "                                                        train_size=None, val_size=None, test_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1531,   77,  106,  661, 1802, 1853,  654,  488, 1954, 1883, 1564,\n",
       "        1782, 2027, 1719,  433, 1998,   22,  224, 1962,  822,  695,  346,\n",
       "         649, 1792,  789,  111, 2047, 1587, 2044, 1488, 1585,  859,  571,\n",
       "        1490, 1870,  560, 1116, 1660, 1041,  283, 1736,  596, 2094, 1631,\n",
       "        1500,   88,  238, 1050,  807, 1834, 1341, 1779, 1262,  905, 1923,\n",
       "        1619, 2074, 1628,  156,  386,  405,  196, 1908,  492,  843,  723,\n",
       "        1115, 1214, 1766,  623, 1114, 1276, 1268, 1649,  271,  265,  990,\n",
       "        1409, 1737, 1596, 1310,  604, 1366,  368, 1992,  400,  573,  329,\n",
       "         950, 1090, 1578, 2078, 1603, 1252, 1440,  181, 1452,  899,  717,\n",
       "         667, 1637, 1325, 1725, 1444,  652, 1007,  192,  605,  630,  640,\n",
       "         436,  171,  882,  628, 1871,  784,  650, 1604,  636, 1971]),\n",
       " array([1332, 1600,   19,   10,  514,  919, 1285,  698,  925,  278,  353,\n",
       "        1565,  627,  610, 1773,  726,  229,  612, 1566,  227, 1156,  616,\n",
       "         318, 1064, 1982, 1713,  140, 1403, 1798,  740,  442,  335, 1752,\n",
       "        1974,  142, 1951, 1016,  757,  764, 1504, 1249, 1726,  212,  416,\n",
       "         369,  409,  404, 1220, 1524, 1389, 2108,  832, 1526,  423,  972,\n",
       "        1076,  457, 2013, 1940,  779,  121, 1580, 1093, 1638,   61,   30,\n",
       "        2054, 1142,  491, 1911, 1459,  187,  951,  520,  415, 1317,  675,\n",
       "        1821, 1745,  960,  730, 1703, 1469,  167,   95,  855,  550, 1157,\n",
       "         809, 1321,  255, 2043, 1364, 1442, 1279,  170,  745,  350, 1994,\n",
       "         387,  774, 1774, 1701, 1599, 1395, 1512,   70, 1358,  431,  148,\n",
       "        1497, 1691,  787, 1539, 1938,  342, 1034,  785, 1915, 1039,  685,\n",
       "        1238, 1196, 1797,  507, 1258,  284, 1082, 1933, 1970, 1734, 1907,\n",
       "         518,  527, 1315,  261, 1833,  863, 1353,   49, 1095, 1126,  881,\n",
       "         773, 1078, 1856, 1181, 1543, 1485,  829,  622,  971,  851, 1561,\n",
       "         201, 2046, 1312, 1362, 1288, 1858, 1054, 1722, 2063,  835,   40,\n",
       "         418, 1790, 1576, 1684, 1976, 1338, 1445, 2105,  602,  639, 1808,\n",
       "        1227,  322, 1337,  618]),\n",
       " array([   0,    1,    2, ..., 2106, 2107, 2109]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj.tocoo(), features.tocsr(), label.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3312x3312 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 4715 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        print(\"Randomly split dataset...\")\n",
    "        dataset_str += '.npz'\n",
    "        dataset_graph = load_npz_to_sparse_graph(dataset_str)\n",
    "\n",
    "        if standardize:\n",
    "            dataset_graph = dataset_graph.standardize()\n",
    "        else:\n",
    "            # dataset_graph = dataset_graph.to_undirected()\n",
    "            dataset_graph = eliminate_self_loops(dataset_graph)\n",
    "\n",
    "\n",
    "        adj, features, labels = dataset_graph.unpack()\n",
    "        labels = binarize_labels(labels)\n",
    "        if not is_binary_bag_of_words(features):\n",
    "            print(\"Converting features of dataset {} to binary bag-of-words representation.\".format(dataset_str[:-4]))\n",
    "            features = to_binary_bag_of_words(features)\n",
    "\n",
    "        # adj matrix needs to be symmetric\n",
    "        if standardize:\n",
    "            assert (adj != adj.T).nnz == 0\n",
    "        # features need to be binary bag-of-word vectors\n",
    "        assert is_binary_bag_of_words(features), f\"Non-binary node_features entry!\"\n",
    "\n",
    "        label = np.where(labels)[1]\n",
    "        idx_train, idx_val, idx_test = get_train_val_test_split(RandomState, labels, train_examples_per_class=20,\n",
    "                                                                val_examples_per_class=30,\n",
    "                                                                test_examples_per_class=None,\n",
    "                                                                train_size=None, val_size=None, test_size=None)\n",
    "\n",
    "        return adj.tocoo(), features.tocsr(), label.astype(np.int32), torch.LongTensor(idx_train), torch.LongTensor(idx_val), torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalized KL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.FloatTensor([0.5, 0.4, 0.1])\n",
    "kl = torch.FloatTensor([1, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.8000, 0.4000])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs * kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
