import argparsefrom Model import *from dataset import *import warningswarnings.simplefilter("ignore")def main(config):    RandomState = np.random.RandomState(config.seed)    adj, feats, label, idx_train, idx_val, idx_test = load_data(config.dataset, RandomState, mode=config.mode,                                                                standardize=config.standardize,                                                                train_num_per_class=config.train_num_per_class,                                                                val_num_per_class=config.val_num_per_class, seed=config.seed)    print(config)    print('num of nodes in adj : {:4d}'.format(adj.shape[0]))    print('num of edges in adj : {:4f}'.format(adj.nnz / 2))    print('num of features     : {:4d}'.format(feats.shape[1]))    print('Is adj symmetric    :      ', (adj != adj.T).sum() == 0)    print('num of training data: {:4d}'.format(len(idx_train)))    print('num of validate data: {:4d}'.format(len(idx_val)))    print('num of test data:     {:4d}'.format(len(idx_test)))    n_class = len(np.unique(label))    print("num of class:         {:4d}".format(n_class))    # return    use_cuda = torch.cuda.is_available()    device = torch.device('cuda' if use_cuda else 'cpu')    print("device:", device)    label = torch.from_numpy(label)    dummy = Engine(feats=feats, adj=adj, n_class=n_class, dataset=config.dataset, labels=label,                   idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, device=device, config=config,                   model=config.model, dim=config.dim,                   dropout=config.dropout, max_iter=config.max_iter, alpha=config.alpha,                   neighbor=config.neighborSizeK, seed=config.seed)if __name__ == '__main__':    parser = argparse.ArgumentParser()    # Model configuration    parser.add_argument('--seed', type=int, default=55, help='random seed of model')    parser.add_argument('--dataset', type=str, default='citeseer',                        help='load dataset cora|citeseer|pubmed|ms_academic_phy')    parser.add_argument('--mode', type=str, default='split', choices=['split', ''], help='load popular or splited datasets')    parser.add_argument('--model', type=str, default='rugcn', help='Model for training')    parser.add_argument('--dim', type=int, default=5, help='dimension of the logits')    parser.add_argument('--dropout', type=float, default=0., help='dropout of the first layers')    parser.add_argument('--max_iter', type=int, default=200, help='maximum epochs')    parser.add_argument('--alpha', type=float, default=1, help='tune-parameter of unlabelled loss')    parser.add_argument('--neighborSizeK', type=int, default=999, help='Controls the number of neighbors of unlabeled nodes')    parser.add_argument('--norm_edges', type=bool, default=True, help='normalize attribution matrix.')    # model setting.    parser.add_argument('--lr', type=float, default=0.05, help='learning rate of model')    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight_decay parameter of adam optimizer')    parser.add_argument('--hidden_size', type=int, default=32, help='hidden layer size of model')    parser.add_argument('--use_relu', type=bool, default=False, help='use relu as activation function')    parser.add_argument('--negative_slope', type=float, default=0.2, help='parameter of leakyed_relu')    parser.add_argument('--standardize', type=bool, default=True, help='standardize some dataset')    parser.add_argument('--train_num_per_class', type=int, default=20, help='training size per class if use data split')    parser.add_argument('--val_num_per_class', type=int, default=30, help='validate size per class if use data split')    parser.add_argument('--normalize_bias', type=bool, default=True, help='whether do L2 loss for bias')    parser.add_argument('--sym_normalize', type=bool, default=False, help='use symmetrically normalize adjacency matrix')    config = parser.parse_args()    main(config)