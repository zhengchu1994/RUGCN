from utils import *import numpy as npimport torch.optim as optimfrom nn import *from Walker import Walkerfrom collections import Counterclass Engine:    def __init__(self, feats, adj, n_class, dataset, labels, idx_train, idx_val, idx_test, device, config, model='rugcn', dim=9, n_hidden=None, dropout=0,                 max_iter=200, alpha=0.5, neighbor=10, seed=0):        # set seed        np.random.seed(seed)        torch.manual_seed(seed)        feats = feats.astype(np.float32)        self.origin_adj = adj.tocsr()        self.feats = normalize(feats)        self.config = config        # normailize adj        if config.sym_normalize:            # Symmetrically normalize adjacency matrix.            self.adj = symnormalize(add_self_loops(adj))        else:            self.adj = normalize(add_self_loops(adj))        self.N, self.D = feats.shape        self.dim = dim        self.n_class = n_class        self.max_iter = max_iter        self.labels = labels.long().to(device)        self.idx_train = idx_train.long().to(device)        self.idx_val = idx_val.long().to(device)        self.idx_test = idx_test.long().to(device)        self.alpha = alpha        self.device = device        if n_hidden is None:            n_hidden = config.hidden_size        self.n_hidden = n_hidden        self.dropout = dropout        self.G = nx.from_scipy_sparse_matrix(adj, create_using=nx.Graph())        self.dataset = dataset        self.Walker = Walker(adj, feats, idx_train, idx_val, idx_test, num_walks=4, walk_length=8, p=7, q=6)        # Using in GPU        self.feats = torch.as_tensor(self.feats.todense()).float().to(device)        self.adj = torch.as_tensor(self.adj.todense()).float().to(device)        # build model        if model == "rugcn":            self.model = Conv.GaussLayer(self.D, n_hidden, dim, n_class, config, dropout, self.alpha).to(device)        else:            print("No model")            return        if config.normalize_bias:            self.Optimizer = optim.Adam(self.model.parameters(), lr=config.lr, weight_decay=config.weight_decay)        else:            print("separatly normalization")            hidden_list = (param for name, param in self.model.named_parameters() if 'hiddenLayer' in name)            mu_list = (param for name, param in self.model.named_parameters() if 'muLayer' in name)            sigma_list = (param for name, param in self.model.named_parameters() if 'sigmaLayer' in name)            parameters = [{'params': hidden_list, 'weight_decay': config.weight_decay},                          {'params': mu_list, 'weight_decay': config.weight_decay},                          {'params': sigma_list, 'weight_decay': config.weight_decay}]            self.Optimizer = optim.Adam(parameters, lr=config.lr)        self.walk_generator(self.Walker)        # self.__dataset_generator(self.idx_train, self.idx_val, self.idx_test, neighbor)        print(self.model)        patience = 20        val_acc = 0        print('models is : ', model)        for epoch in range(1, self.max_iter + 1):            self.model.train()            self.Optimizer.zero_grad()            if model == 'rugcn':                self.muv, self.sigmav, self.logitsv = self.model(self.feats, self.adj)                loss = self.model.build_loss2(self.train_triples, self.probs, self.idx_train, self.labels[self.idx_train])                # loss = self.model.build_loss(self.train_triples, self.idx_train, self.labels[self.idx_train])            loss.backward()            self.Optimizer.step()            loss_value = loss.item()            acc_train = accuracy(self.logitsv[self.idx_train], self.labels[self.idx_train])            acc_val = evaluate(self.model, self.feats, self.adj, self.labels, self.idx_val)            acc_test = evaluate(self.model, self.feats, self.adj, self.labels, self.idx_test)            if acc_val > val_acc:                val_acc = acc_val                patience = 10            else:                patience -= 1            if patience % 1 == 0:                print("Patience is : ", patience, "acc_test : ", acc_test)            if patience == 0:                break        print(config)    def walk_generator(self, walker):        walks = walker.walks        unnormalized_probs = []        edges = []        for st_node, dest_nodes in walks.items():            sigle_node_unnormalized_prob = []            for dest_node, ct in Counter(dest_nodes).items():                sigle_node_unnormalized_prob.append(ct)                edges.append([st_node, dest_node])            sigle_node_unnormalized_prob = np.array(sigle_node_unnormalized_prob)            unnormalized_probs.append(sigle_node_unnormalized_prob / sigle_node_unnormalized_prob.sum())        self.train_triples = torch.LongTensor(edges)        self.probs = torch.FloatTensor(np.concatenate(unnormalized_probs))    def __dataset_generator(self, idx_train, idx_val, idx_test, neighbor):        print("neighbor size is : ", neighbor)        idx_train = idx_train.tolist()        idx_val = idx_val.tolist()        idx_test = idx_test.tolist()        # 未标签节点与未标签节点间的边: 验证集节点        uuvaledges = []        for item in idx_val:            neibors = list(self.G.neighbors(item))            if len(neibors) != 0 and len(neibors) <= neighbor:                for neibor in neibors:                    if neibor not in idx_train:                        uuvaledges.append([item, neibor])        # 未标签节点与未标签节点间的边: 测试集节点        uutestedges = []        for item in idx_test:            neibors = list(self.G.neighbors(item))            if len(neibors) != 0 and len(neibors) <= neighbor:                for neibor in neibors:                    if neibor not in idx_train:                        uutestedges.append([item, neibor])        self.train_triples = torch.LongTensor(np.concatenate([uuvaledges, uutestedges]))        print("self.train_triples:", self.train_triples)        self.index = self.train_triples.transpose(0, 1).contiguous()        self.src = torch.as_tensor(list(set(self.index[0, :].tolist())))        # self.train_triples = self.train_triples[torch.randperm(len(self.train_triples))]        print("training edges size :", self.train_triples.size(), '\n')    def __dataset_generator_each_node(self, idx_train, idx_val, idx_test, neighbor):        idd = torch.cat([idx_val.to(dtype=torch.long), idx_test.to(dtype=torch.long)]).tolist()        p_smat = self.origin_adj[idd]        ans = [(i, p_smat[i].nonzero()[1]) for i in range(p_smat.shape[0]) if p_smat[i].sum() != 0]        edges = list()        for idx, idy in ans:            if len(idy) <= neighbor:                [edges.append((idx, _idy)) for _idy in idy if _idy not in idx_train]                # le = np.random.choice(idy, size=neighbor, replace=True)        self.train_triples = torch.LongTensor(edges).to(self.device)        self.train_triples = self.train_triples[torch.randperm(len(self.train_triples))]        print(self.train_triples[0])        print("training edges size :", self.train_triples.size(), '\n')